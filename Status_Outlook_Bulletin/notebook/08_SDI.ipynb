{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11871efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6a0096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estaciones encontradas: ['101', '1170', '1190', '1230', '1330', '140', '1400', '1410', '150', '1740', '17430', '21530', '22060', '26662', '26665', '26669', '26676', '440', '511', '520', '531', '551', '591', '651', '820', '970']\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración Inicial ---\n",
    "k_values = [3, 6]  # Escalas temporales a procesar\n",
    "m = 4  # Mes de inicio año hidrológico (Abril)\n",
    "data_folder = 'd:/GitHub/Sala_Situacion_Dinagua/Status_Outlook_Bulletin/stations/data'\n",
    "\n",
    "# Percentage of missing data\n",
    "max_pct_missing = 50\n",
    "\n",
    "# Obtener lista de estaciones disponibles\n",
    "import glob\n",
    "station_files = glob.glob(os.path.join(data_folder, '*.csv'))\n",
    "stations = [os.path.basename(f).replace('.csv', '') for f in sorted(station_files)]\n",
    "print(f\"Estaciones encontradas: {stations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6165daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_station_data(station_name, data_folder):\n",
    "    \"\"\"\n",
    "    Cargar datos de descarga diaria para una estación.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station_name : str\n",
    "        Nombre/ID de la estación\n",
    "    data_folder : str\n",
    "        Ruta de la carpeta con los datos\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame con datos diarios, o None si hay error\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(data_folder, f'{station_name}.csv')\n",
    "    try:\n",
    "        discharge_daily = pd.read_csv(\n",
    "            input_file,\n",
    "            parse_dates=['Fecha'],\n",
    "            index_col=\"Fecha\",\n",
    "            dayfirst=True,\n",
    "            na_values=\"NA\"\n",
    "        )\n",
    "        return discharge_daily\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando estación {station_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bac2621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_discharge_to_monthly(discharge_daily, max_pct_missing=50):\n",
    "    \"\"\"\n",
    "    Convert daily discharge data to monthly discharge data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    discharge_daily : pd.DataFrame\n",
    "        Daily discharge dataframe with datetime index\n",
    "    max_pct_missing : float\n",
    "        Maximum percentage of missing data allowed (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Monthly discharge dataframe with columns: Fecha and Caudal\n",
    "    \"\"\"\n",
    "    # Re-index to complete date range\n",
    "    date_range = pd.date_range(start=discharge_daily.index[0], \n",
    "                               end=discharge_daily.index[-1], \n",
    "                               freq='D')\n",
    "    discharge_daily = discharge_daily.reindex(date_range, fill_value=None)\n",
    "    discharge_daily.index.name = 'Fecha'\n",
    "    \n",
    "    # Standardize column name\n",
    "    discharge_daily.columns = ['Caudal']\n",
    "    \n",
    "    # Convert daily to monthly (mean with missing data threshold)\n",
    "    df = discharge_daily.resample('M', closed=\"right\").apply(\n",
    "        lambda x: x.mean() if x.isnull().sum()*100/len(x) < max_pct_missing else np.nan\n",
    "    )\n",
    "    \n",
    "    # Reset index to make Fecha a column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68987dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sdi_for_station(station_name, k, data_folder, m=4, max_pct_missing=50):\n",
    "    \"\"\"\n",
    "    Procesar SDI para una estación y escala temporal específicas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station_name : str\n",
    "        Nombre/ID de la estación\n",
    "    k : int\n",
    "        Escala temporal en meses\n",
    "    data_folder : str\n",
    "        Ruta de la carpeta con los datos\n",
    "    m : int\n",
    "        Mes de inicio del año hidrológico\n",
    "    max_pct_missing : float\n",
    "        Máximo porcentaje de datos faltantes permitido\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame con resultados SDI o None si hay error\n",
    "    \"\"\"\n",
    "    # Cargar datos diarios\n",
    "    discharge_daily = load_station_data(station_name, data_folder)\n",
    "    if discharge_daily is None:\n",
    "        return None\n",
    "    \n",
    "    # Convertir a mensual\n",
    "    date_range = pd.date_range(start=discharge_daily.index[0], \n",
    "                               end=discharge_daily.index[-1], \n",
    "                               freq='D')\n",
    "    discharge_daily = discharge_daily.reindex(date_range, fill_value=None)\n",
    "    discharge_daily.index.name = 'Fecha'\n",
    "    discharge_daily.columns = ['Caudal']\n",
    "    \n",
    "    df = discharge_daily.resample('M', closed=\"right\").apply(\n",
    "        lambda x: x.mean() if x.isnull().sum()*100/len(x) < max_pct_missing else np.nan\n",
    "    )\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Preprocesamiento\n",
    "    df['cumCaudal'] = df['Caudal'].rolling(window=k).sum()\n",
    "    df['lnCaudal'] = np.log(df['cumCaudal'])\n",
    "    \n",
    "    # Etiquetas de meses\n",
    "    df['StartMonth'] = (df['Fecha'] - pd.DateOffset(months=k-1)).dt.strftime('%b')\n",
    "    df['EndMonth'] = df['Fecha'].dt.strftime('%b')\n",
    "    df['ScaleMonth'] = df['StartMonth'] + \"-\" + df['EndMonth']\n",
    "    \n",
    "    # Inicializar columnas de resultados\n",
    "    df['SDI'] = np.nan\n",
    "    df['log_SDI'] = np.nan\n",
    "    df['Gamma_SDI'] = np.nan\n",
    "    \n",
    "    # Cálculo del SDI por grupo de meses\n",
    "    iterations = df['ScaleMonth'].unique()\n",
    "    \n",
    "    for month_group in iterations:\n",
    "        mask = (df['ScaleMonth'] == month_group) & (df['cumCaudal'].notna())\n",
    "        \n",
    "        if not mask.any():\n",
    "            continue\n",
    "            \n",
    "        subset = df.loc[mask, 'cumCaudal']\n",
    "        subset_ln = df.loc[mask, 'lnCaudal']\n",
    "        \n",
    "        # 1. SDI Estándar\n",
    "        df.loc[mask, 'SDI'] = (subset - subset.mean()) / subset.std()\n",
    "        \n",
    "        # 2. SDI Log-Normal\n",
    "        df.loc[mask, 'log_SDI'] = (subset_ln - subset_ln.mean()) / subset_ln.std()\n",
    "        \n",
    "        # 3. SDI Gamma\n",
    "        try:\n",
    "            shape, loc, scale = stats.gamma.fit(subset, floc=0)\n",
    "            cdf_vals = stats.gamma.cdf(subset, shape, loc, scale)\n",
    "            cdf_vals = np.clip(cdf_vals, 0.0001, 0.9999)\n",
    "            df.loc[mask, 'Gamma_SDI'] = stats.norm.ppf(cdf_vals)\n",
    "        except:\n",
    "            df.loc[mask, 'Gamma_SDI'] = np.nan\n",
    "    \n",
    "    # Año Hidrológico\n",
    "    def get_hydro_year(date):\n",
    "        return date.year if date.month >= m else date.year - 1\n",
    "    \n",
    "    df['WYear'] = df['Fecha'].apply(get_hydro_year)\n",
    "    df['Año_hidrologico'] = df['WYear'].astype(str) + \"-\" + (df['WYear'] + 1).astype(str)\n",
    "    \n",
    "    # Preparar exportación\n",
    "    data_export = df[['Fecha', 'Año_hidrologico', 'ScaleMonth', 'SDI', 'log_SDI', 'Gamma_SDI']].copy()\n",
    "    data_export.columns = [\"Fecha\", \"Año_hidrologico\", \"Escala\", \"SDI\", \"LogSDI\", \"GammaSDI\"]\n",
    "    data_export[['SDI', 'LogSDI', 'GammaSDI']] = data_export[['SDI', 'LogSDI', 'GammaSDI']].round(2)\n",
    "    \n",
    "    return data_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1fd32d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Procesando escala k=3 meses\n",
      "============================================================\n",
      "  Procesando estación 101... ✓ Guardado\n",
      "  Procesando estación 1170... ✓ Guardado\n",
      "  Procesando estación 1190... ✓ Guardado\n",
      "  Procesando estación 1230... ✓ Guardado\n",
      "  Procesando estación 1330... ✓ Guardado\n",
      "  Procesando estación 140... ✓ Guardado\n",
      "  Procesando estación 1400... ✓ Guardado\n",
      "  Procesando estación 1410... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DINAGUA\\anaconda3\\envs\\HydroSOS\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Guardado\n",
      "  Procesando estación 150... ✓ Guardado\n",
      "  Procesando estación 1740... ✓ Guardado\n",
      "  Procesando estación 17430... ✓ Guardado\n",
      "  Procesando estación 21530... ✓ Guardado\n",
      "  Procesando estación 22060... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DINAGUA\\anaconda3\\envs\\HydroSOS\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Guardado\n",
      "  Procesando estación 26662... ✓ Guardado\n",
      "  Procesando estación 26665... ✓ Guardado\n",
      "  Procesando estación 26669... ✓ Guardado\n",
      "  Procesando estación 26676... ✓ Guardado\n",
      "  Procesando estación 440... ✓ Guardado\n",
      "  Procesando estación 511... ✓ Guardado\n",
      "  Procesando estación 520... ✓ Guardado\n",
      "  Procesando estación 531... ✓ Guardado\n",
      "  Procesando estación 551... ✓ Guardado\n",
      "  Procesando estación 591... ✓ Guardado\n",
      "  Procesando estación 651... ✓ Guardado\n",
      "  Procesando estación 820... ✓ Guardado\n",
      "  Procesando estación 970... ✓ Guardado\n",
      "\n",
      "============================================================\n",
      "Procesando escala k=6 meses\n",
      "============================================================\n",
      "  Procesando estación 101... ✓ Guardado\n",
      "  Procesando estación 1170... ✓ Guardado\n",
      "  Procesando estación 1190... ✓ Guardado\n",
      "  Procesando estación 1230... ✓ Guardado\n",
      "  Procesando estación 1330... ✓ Guardado\n",
      "  Procesando estación 140... ✓ Guardado\n",
      "  Procesando estación 1400... ✓ Guardado\n",
      "  Procesando estación 1410... ✓ Guardado\n",
      "  Procesando estación 150... ✓ Guardado\n",
      "  Procesando estación 1740... ✓ Guardado\n",
      "  Procesando estación 17430... ✓ Guardado\n",
      "  Procesando estación 21530... ✓ Guardado\n",
      "  Procesando estación 22060... ✓ Guardado\n",
      "  Procesando estación 26662... ✓ Guardado\n",
      "  Procesando estación 26665... ✓ Guardado\n",
      "  Procesando estación 26669... ✓ Guardado\n",
      "  Procesando estación 26676... ✓ Guardado\n",
      "  Procesando estación 440... ✓ Guardado\n",
      "  Procesando estación 511... ✓ Guardado\n",
      "  Procesando estación 520... ✓ Guardado\n",
      "  Procesando estación 531... ✓ Guardado\n",
      "  Procesando estación 551... ✓ Guardado\n",
      "  Procesando estación 591... ✓ Guardado\n",
      "  Procesando estación 651... ✓ Guardado\n",
      "  Procesando estación 820... ✓ Guardado\n",
      "  Procesando estación 970... ✓ Guardado\n",
      "\n",
      "============================================================\n",
      "RESUMEN DE PROCESAMIENTO\n",
      "============================================================\n",
      "Estación Escala  Registros Estado\n",
      "     101     3m        552     OK\n",
      "    1170     3m        551     OK\n",
      "    1190     3m        665     OK\n",
      "    1230     3m        540     OK\n",
      "    1330     3m        552     OK\n",
      "     140     3m        533     OK\n",
      "    1400     3m        545     OK\n",
      "    1410     3m        459     OK\n",
      "     150     3m        539     OK\n",
      "    1740     3m        490     OK\n",
      "   17430     3m        581     OK\n",
      "   21530     3m        401     OK\n",
      "   22060     3m        612     OK\n",
      "   26662     3m        521     OK\n",
      "   26665     3m        521     OK\n",
      "   26669     3m        521     OK\n",
      "   26676     3m        521     OK\n",
      "     440     3m        552     OK\n",
      "     511     3m        506     OK\n",
      "     520     3m        548     OK\n",
      "     531     3m        514     OK\n",
      "     551     3m        551     OK\n",
      "     591     3m        483     OK\n",
      "     651     3m        509     OK\n",
      "     820     3m        436     OK\n",
      "     970     3m        551     OK\n",
      "     101     6m        552     OK\n",
      "    1170     6m        551     OK\n",
      "    1190     6m        665     OK\n",
      "    1230     6m        540     OK\n",
      "    1330     6m        552     OK\n",
      "     140     6m        533     OK\n",
      "    1400     6m        545     OK\n",
      "    1410     6m        459     OK\n",
      "     150     6m        539     OK\n",
      "    1740     6m        490     OK\n",
      "   17430     6m        581     OK\n",
      "   21530     6m        401     OK\n",
      "   22060     6m        612     OK\n",
      "   26662     6m        521     OK\n",
      "   26665     6m        521     OK\n",
      "   26669     6m        521     OK\n",
      "   26676     6m        521     OK\n",
      "     440     6m        552     OK\n",
      "     511     6m        506     OK\n",
      "     520     6m        548     OK\n",
      "     531     6m        514     OK\n",
      "     551     6m        551     OK\n",
      "     591     6m        483     OK\n",
      "     651     6m        509     OK\n",
      "     820     6m        436     OK\n",
      "     970     6m        551     OK\n",
      "\n",
      "Total procesados: 52/52\n",
      "Archivos guardados en: d:/GitHub/Sala_Situacion_Dinagua/Status_Outlook_Bulletin/stations/output_sdi/csv\n"
     ]
    }
   ],
   "source": [
    "# --- Procesamiento Automatizado de Todas las Estaciones y Escalas ---\n",
    "\n",
    "# Crear carpeta de salida si no existe\n",
    "output_base = 'd:/GitHub/Sala_Situacion_Dinagua/Status_Outlook_Bulletin/stations/output_sdi/csv'\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# Procesar todas las estaciones y escalas\n",
    "results_summary = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Procesando escala k={k} meses\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for station in stations:\n",
    "        print(f\"  Procesando estación {station}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            data_export = process_sdi_for_station(station, k, data_folder, m=m, max_pct_missing=max_pct_missing)\n",
    "            \n",
    "            if data_export is not None and len(data_export) > 0:\n",
    "                # Guardar archivo\n",
    "                output_path = os.path.join(output_base, f\"{k}month_CompleteSDI_{station}.txt\")\n",
    "                data_export.to_csv(output_path, index=False, sep=\",\", na_rep=\"\")\n",
    "                print(f\"✓ Guardado\")\n",
    "                results_summary.append({\n",
    "                    'Estación': station,\n",
    "                    'Escala': f\"{k}m\",\n",
    "                    'Registros': len(data_export),\n",
    "                    'Estado': 'OK'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"✗ Sin datos válidos\")\n",
    "                results_summary.append({\n",
    "                    'Estación': station,\n",
    "                    'Escala': f\"{k}m\",\n",
    "                    'Registros': 0,\n",
    "                    'Estado': 'Sin datos'\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {str(e)[:50]}\")\n",
    "            results_summary.append({\n",
    "                'Estación': station,\n",
    "                'Escala': f\"{k}m\",\n",
    "                'Registros': 0,\n",
    "                'Estado': f'Error: {str(e)[:30]}'\n",
    "            })\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUMEN DE PROCESAMIENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Estadísticas finales\n",
    "total_exitosos = len([r for r in results_summary if r['Estado'] == 'OK'])\n",
    "total_procesados = len(stations) * len(k_values)\n",
    "print(f\"\\nTotal procesados: {total_exitosos}/{total_procesados}\")\n",
    "print(f\"Archivos guardados en: {output_base}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HydroSOS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
